{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42119540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7160bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DIPnet import DIPNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b1bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = DIPNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c06eb9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DIPNet(\n",
       "  (conv_1): Conv2d(3, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block_1): RRFB(\n",
       "    (c1_r): Conv2d(44, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c2_r): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c3_r): Conv2d(38, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c5): Conv2d(44, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (esa): ESA(\n",
       "      (conv1): Conv2d(44, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_f): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(16, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (act): LeakyReLU(negative_slope=0.05, inplace=True)\n",
       "  )\n",
       "  (block_2): RRFB(\n",
       "    (c1_r): Conv2d(44, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c2_r): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c3_r): Conv2d(38, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c5): Conv2d(44, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (esa): ESA(\n",
       "      (conv1): Conv2d(44, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_f): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(16, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (act): LeakyReLU(negative_slope=0.05, inplace=True)\n",
       "  )\n",
       "  (block_3): RRFB(\n",
       "    (c1_r): Conv2d(44, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c2_r): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c3_r): Conv2d(38, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c5): Conv2d(44, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (esa): ESA(\n",
       "      (conv1): Conv2d(44, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_f): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(16, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (act): LeakyReLU(negative_slope=0.05, inplace=True)\n",
       "  )\n",
       "  (block_4): RRFB(\n",
       "    (c1_r): Conv2d(44, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c2_r): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c3_r): Conv2d(38, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (c5): Conv2d(44, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (esa): ESA(\n",
       "      (conv1): Conv2d(44, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv_f): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv4): Conv2d(16, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (act): LeakyReLU(negative_slope=0.05, inplace=True)\n",
       "  )\n",
       "  (conv_2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (upsampler): Sequential(\n",
       "    (0): Conv2d(44, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): PixelShuffle(upscale_factor=4)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = \"/home/anansupercuteeeee/Music/sr/super-resolution-lw/trained_model/student_dipnet_distilled.pth\"\n",
    "batch_size = 4    # just a random number\n",
    "\n",
    "# Initialize model with the pretrained weights\n",
    "map_location = lambda storage, loc: storage\n",
    "if torch.cuda.is_available():\n",
    "    map_location = None\n",
    "torch_model.load_state_dict(torch.load(pretrained_model, map_location=map_location))\n",
    "\n",
    "# set the model to inference mode\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f24d18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, 3, 224, 224, requires_grad=True)\n",
    "torch_out = torch_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f7d61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(torch_model,\n",
    "                  x,  # dummy input\n",
    "                  \"super_resolution.onnx\",\n",
    "                  export_params=True,\n",
    "                  opset_version=11,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names=['input'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={\n",
    "                      'input': {0: 'batch_size', 2: 'height', 3: 'width'},\n",
    "                      'output': {0: 'batch_size', 2: 'height_up', 3: 'width_up'}\n",
    "                  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e095cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"super_resolution.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63d0b630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "228e11f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# Export ONNX\n",
    "input_tensor = torch.randn(1, 3, 64, 64)  # hoặc input bạn đang dùng\n",
    "torch_out = torch_model(input_tensor)\n",
    "\n",
    "# Chạy ONNX\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_tensor)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37b014a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference of Pytorch model used 2.2329940795898438 seconds\n",
      "Inference of ONNX model used 1.4303443431854248 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x = torch.randn(1, 3, 512, 720, requires_grad=True)\n",
    "\n",
    "start = time.time()\n",
    "torch_out = torch_model(x)\n",
    "end = time.time()\n",
    "print(f\"Inference of Pytorch model used {end - start} seconds\")\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "start = time.time()\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "end = time.time()\n",
    "print(f\"Inference of ONNX model used {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eef5835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Providers: ['CPUExecutionProvider']\n",
      "Used Execution Provider: CPUExecutionProvider\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution Providers:\", ort_session.get_providers())\n",
    "print(\"Used Execution Provider:\", ort_session.get_providers()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c82b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Phải là True\n",
    "print(x.device)                   # Phải là \"cuda:0\" nếu muốn so sánh đúng tốc độ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf69b185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Export ONNX thành công!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Giả sử model đã được load và set .eval()\n",
    "model = torch_model.eval().to(\"cuda\")\n",
    "\n",
    "# Dummy input giống khi inference\n",
    "dummy_input = torch.randn(1, 3, 512, 512).to(\"cuda\")\n",
    "\n",
    "# Export\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input,\n",
    "    \"super_resolution.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=17,                # dùng opset mới\n",
    "    do_constant_folding=True,       # tối ưu biểu thức hằng\n",
    "    input_names=['input'], \n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # cho phép batch size thay đổi\n",
    ")\n",
    "print(\"✅ Export ONNX thành công!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928ecc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-03 23:14:36.668860216 [W:onnxruntime:, session_state.cc:1263 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-03 23:14:36.668878562 [W:onnxruntime:, session_state.cc:1265 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model = onnx.load(\"super_resolution.onnx\")\n",
    "onnx.checker.check_model(onnx_model)  # kiểm tra cấu trúc\n",
    "\n",
    "ort_session = ort.InferenceSession(\"super_resolution.onnx\", providers=[\"CUDAExecutionProvider\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ebbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c1bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.10.0.31\n"
     ]
    }
   ],
   "source": [
    "print(tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c01e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "ort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\")\n",
    "print(ort_session.get_inputs()[0].name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27eff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%trtexec` not found.\n"
     ]
    }
   ],
   "source": [
    "%trtexec --version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fea066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
